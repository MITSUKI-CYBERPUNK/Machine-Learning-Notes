# 3 单变量线性回归Univariate Linear Regression和代价(成本)函数Cost Function

## 3.1 单变量线性回归Univariate Linear

eg.根据房屋大小预测住房价格

![image-20240130094047742](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130094047742.png)

这是一个监督学习的线性回归问题。我们的数据集称为训练集。

![image-20240130095058677](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130095058677.png)



### 3.1.1 训练集标准符号

**整个课程中将用小写的$m$来表示训练样本的数目**

**m 代表训练集中实例的数量**

**x 代表特征/输入变量** input/feature

**y 代表目标变量/输出变量** output/target

**(x,y) 代表训练集中的实例**

**(x^(i)^,y^(i)^)代表第$i$ 个观察实例**

**代表学习算法的解决方案或函数也称为假设（hypothesis）**

![image-20240130100808795](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130100808795.png)

$h$表示一个函数，输入是房屋尺寸大小，根据输入的$x$值来得出$y$值，$y$ 值对应房子的价格 因此，$h$ 是一个从$x$到$y$的函数映射。

要解决房价预测问题，我们实际上是要将训练集“喂”给我们的学习算法，进而学习得到一个假设$h$，然后将我们要预测的房屋的尺寸作为输入变量输入给$h$，预测出该房屋的交易价格作为输出变量输出为结果。那么，对于我们的房价预测问题，我们该如何表达$h$ ？

一种可能的表达方式为：$h_\theta \left( x \right)=\theta_{0} + \theta_{1}x$，因为只**含有一个特征/输入变量**，因此这样的问题叫作**单变量线性回归问题**。



**y是目标，训练集中的实际真实值**

**y-hat是对y的估计或预测**



## 3.2 代价（成本）函数Cost Function

### 3.3.1 数学定义

在线性回归的基础上，我们继续研究：

![image-20240130100838357](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130100838357.png)

我们现在要做的便是为我们的模型选择合适的**参数**（**parameters**）$\theta_{0}$ 和 $\theta_{1}$，在房价问题这个例子中便是直线的斜率k和在$y$ 轴上的截距b。

我们选择的**参数决定了我们得到的直线相对于我们的训练集的准确程度**，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是**建模误差**（**modeling error**）。**可以通过不断调整改进参数来减小建模误差。**

![image-20240130101033459](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130101033459.png)

我们的**目标便是选择出可以使得建模误差的平方和能够最小的模型参数。 即使得代价函数 $J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$最小。**（机器学习中再/2使计算更为简洁，此时成本函数仍然有效）



**参数（Parameters）有时也被称为系数（Coefficients）或权重（Weights）。**

我们绘制一个等高线图，三个坐标分别为$\theta_{0}$和$\theta_{1}$ 和$J(\theta_{0}, \theta_{1})$：

![image-20240130101451864](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130101451864.png)

则可以看出在三维空间中存在一个使得$J(\theta_{0}, \theta_{1})$最小的点。

**代价函数也被称作平方误差函数，有时也被称为平方误差代价函数**。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段。



### 3.3.2 代价函数的直观理解

![image-20240130101912818](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130101912818.png)

代价函数的样子:

 ![image-20240130104110485](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130104110485.png)

3D碗形曲面图：碗底使代价函数最小，最适合预测

![image-20240130101451864](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130101451864.png)

等值线图：（对3D碗形曲面图底部取3D表面并水平切片）

![image-20240130102027703](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130102027703.png)

通过这些图形，我希望你能更好地理解这些代价函数$ J$所表达的值是什么样的，它们对应的假设是什么样的，以及什么样的假设对应的点，更接近于代价函数$J$的最小值。

当然，我们真正需要的是一种有效的算法，能够自动地找出这些使代价函数$J$取最小值的参数$\theta_{0}$和$\theta_{1}$来。

我们也不希望编个程序把这些点画出来，然后人工的方法来读出这些点的数值，这很明显不是一个好办法。我们会遇到更复杂、更高维度、更多参数的情况，而这些情况是很难画出图的，因此更无法将其可视化，因此我们真正需要的是编写程序来找出这些最小化代价函数的$\theta_{0}$和$\theta_{1}$的值，在下一节视频中，我们将介绍一种算法，能够**自动地找出能使代价函数$J$最小化的参数$\theta_{0}$和$\theta_{1}$的值**。