# 3 线性回归
[[3 单变量线性回归Univariate Linear Regression和代价(成本)函数Cost Function]]
## 3.1 最小二乘法(矩阵表达)及其几何意义
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240318200629.png) ^5d5dba

### 3.1.1 之于每个样本
+ x<sub>i</sub>指的是p维列向量，y<sub>i</sub>是数值，因此后面x的转置实则是列向量的转置。X是矩阵，便于描述

![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240319173557.png)
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240319173640.png)

+ 结合Ng的解释，模型直线与样本真实值的差值的平方和，可以用来表示拟合度(误差函数)。我们的**目标便是选择出可以使得建模误差的平方和能够最小的模型参数（求最优W使得L最小）

+ ω<sup>T</sup>可以理解为用矩阵表示的**参数**
+ 1/2是为了简化运算，不影响代价函数
+ 平方和而不是和
	+ 对异常值更加**敏感**
	+ 平方和的最小化问题有一个封闭解，也就是说，可以通过求解导数为零的方程来直接得到最小化平方和的参数解。封闭解通常更容易**计算**，并且在一些情况下可以提供更多关于数据集特性的信息
	+ 平方和在**统计**学中也有一些良好的性质，例如在回归分析中，最小二乘估计的参数具有最小方差的特性，这使得基于最小二乘法得到的估计结果在统计推断中具有较好的性能。

+ 之所以展开后得到行向量x列向量的形式，我们可以结合[[图解线性代数]]，从结果来看，这样得到一个值而不是一个列向量。（**其实二者等同，只是后者写为转置形式**）同理，**后者转置则为前者**。另外，**转置逆序**

+ 因为损失函数为实数，因此每一项都是实数
+ argmin()函数：返回使输入函数达到最小值的参数值，而不是最小值本身。我们这里通过**对矩阵求偏导**来实现
+ 用到的矩阵求导公式：（矩阵的微分与迹）![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240318194038.png)
+ 乘以逆矩阵可以理解为”除以“矩阵

+ 也可以理解为伪逆 乘 Y，打包好了有时会比较方便（其实称为左逆会更合适）
+ 公式理解：代价函数可以视为总的误差，而总误差是分散在n个样本点上的，这里结合Ng的图理解会更好 ![image-20240130104110485](https://aquazone.oss-cn-guangzhou.aliyuncs.com/image-20240130104110485.png)

### 3.1.2 几何角度——维度
+ 调换一下，Y向量不一定就在X各列向量张成的空间上，有可能存在很多噪声
+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240319173927.png)
	其实这就是列空间，X的所有列向量组成了一个p维空间
+ **最小二乘法的几何意义**：在P维空间中找到一个向量，使其离平面最近（类似上图中二维的描述），则其投影则为我们要找的损失函数。但每一个向量Y会垂直于每一维的平面，它们投影的和即为我们的**代价函数**，我们的目的就是**使这个投影和最小**。（热泪盈眶！太精彩了！！！）
+ 根据空间垂直关系，我们能再次求出代价函数的解析解。

### 3.1.3 总结
+ 第一种角度是把误差分散在了每一个数据点上
+ 第二种角度是把误差分散在了X的每一个列向量上， P个维度上
+ 不同的角度得到了同样的结果，殊途同归

## 3.2 概率角度：最小二乘法 高斯噪声与MLE
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240318202811.png)

+ 高斯分布其实就是**正态分布**
+ 当数据都在一条直线上时是最完美的情况，误差为0 。但现实中不可能出现这种情况，因为数据都带有一定的**噪声**
	+ 噪声通常是指干扰或混淆观测结果或数据的随机扰动或不确定性。在概率论和统计学中，噪声通常被建模为随机变量，并使用概率分布来刻画其统计特性。常见的噪声模型包括高斯噪声（正态分布）、泊松噪声、伯努利噪声等。

+ **y|x,ω** ： 在参数为ω的模型中，y在x的条件下所服从的概率分布
+ 这里用到高中的均值方差公式与极大似然估计MLE 


## 3.3 正则化：Ridge 频率角度
[[8 正则化Regularization]]
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240321163652.png)

+ **过拟合**解决方法：
	+ 加数据
	+ 降维/特征选择/特征提取PCA
	+ 正则化：保留所有的特征，但是减少参数的大小（magnitude）

+ 正则化的框架：![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240321163958.png)

+ 惩罚函数其实就是为函数加一个约束
 + 两种正则化方式，前者为范数，后者为二范数，我们着重分析一下**岭回归**，即**权值衰减**
	 + **推导步骤**：![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240321164916.png)

+ [[8 正则化Regularization#^c170e1]]减小高次项系数，减小它们的影响，改善过拟合
+ 正则化：![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240321164457.png)

## 3.4 正则化：Ridge 贝叶斯角度
+ MAP为贝叶斯学派常用的参数估计方法，他们认为模型参数服从某种潜在分布。  
	+ 其首先对参数有一个预先估计，然后根据所给数据对预估计进行不断调整，因此同一事件，先验不同则事件状态不同  
	+ 先验假设较为靠谱时有显著的效果，当数据较少时，先验对模型的参数有主导作用，随着数据的增加，真实数据样例将占据主导地位
+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240321165906.png)
  ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240321165921.png)
  ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240321165947.png)
+ 我们发现这与Ridge正则化后的Loss Function一致，这说明**加入了正则项的最小二乘估计与包含服从高斯分布的噪声和先验的MAP是等价的**
+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240321170115.png)

+ 注：
	+ Loss Function是定义在单个样本上的，算的是一个样本的误差
	+ Cost Function 是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均
	+ cost 就是所有loss的平均值

+ 线性回归(Linear Regression)在统计机器学习中占据核心地位，其有三大特性：**线性**、**全局性**和**数据未加工性**
+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240325165304.png)
+ 线性分为**属性线性**、**全局线性**和**系数线性**，将其某一种线性改为非线性，即可得到一种新的模型：
	1. 线性中，若**将属性改为非线性**，即存在x<sup>i</sup>(i>1) ，则为**特征转换（多项式回归）**  
		若**将全局线性改为非线性**，即加入激活函数，使其输出为非线性，则为**线性分类**  
		若**将系数线性改为非线性**，即系数是会变化的，则其为**神经网络（反向传播）**  
	2. 若**将全局性打破**，即不根据所有点的情况回归，而是将数据分为几个段（将特征空间划分为一个一个的小区域），对各个段进行回归，则是**线性样条回归**或**决策树**  
	3. 若**将数据未加工性打破**，即在数据输入之前进行降维处理，则为**PCA或流形**
+ 可以看出**线性回归是统计机器学习的核心与基础，将其某个性质进行改变即可得到一个新的模型**

## 3.5 评估标准
+ **均方误差 (MSE)**：均方误差是预测值与实际值之间差异的平方的平均值。它衡量了模型的预测与真实值之间的平均偏差。较低的 MSE 表示更好的拟合。

   ```
   MSE = 1/n ∑(y_i - ŷ_i)^2
   ```

   其中：
   - *n* 是样本数量。
   - *y_i* 是第 i 个样本的真实值。
   - *ŷ_i* 是第 i 个样本的预测值。

 + **均方根误差 (RMSE)**：均方根误差是 MSE 的平方根。它与 MSE 有 similar 的解释，但使用相同的尺度来表示误差，使其解释起来更方便。

   ```
   RMSE = sqrt(MSE)
   ```

+ **平均绝对误差 (MAE)**：平均绝对误差是预测值与实际值之间绝对差值的平均值。它提供了模型预测误差的平均程度。

   ```
   MAE = 1/n ∑|y_i - ŷ_i|
   ```

+ **R-平方 (R^2)**：R-平方也称为决定系数，它表示模型解释的响应变量的方差比例。它表明了线性回归模型对数据拟合的好坏。 R^2 的值介于 0 和 1 之间，值越接近 1 表示拟合越好。

   ```
   R^2 = 1 - (SS_res / SS_tot)
   ```

   其中：
   - *SS_res* 是残差平方和。
   - *SS_tot* 是总平方和。

+ **调整后的 R-平方 (adjusted R^2)**：当回归模型包含多个自变量时，调整后的 R-平方可以考虑自变量数量的影响。它惩罚了过多的自变量，从而更适合比较不同数量的自变量的模型。

+ **残差分析**：残差分析涉及检查模型残差（预测值与真实值之间的差异）的分布和模式。理想情况下，残差应该呈现随机分布，没有明显的模式。残差分析可以帮助发现模型中的异常值、异方差或自相关等问题。
