+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240401154929.png)

## 11.5 整合
+ 决策树学习：
	1. 从根节点处的所有训练示例开始
	2. 计算所有可能特征的信息增益，并选择要拆分的特征，从而提供最高的信息增益
	3. 根据所选特征将数据集拆分为两个子集，并创建树的左右分支
	4. 在树的左右分支上重复拆分过程直到满足终止条件：
		+ 当熵=0，纯度百分百
		+ 达到最大深度
		+ 信息增益小于阈值
		+ 示例数量小于阈值
+ 构建决策树的方式：**通过构建较小的子决策树然后将它们放在一起来构建整体决策树**，编程上通过**递归**实现

+ **ID3决策树标准算法步骤**:![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240401155022.png)

## 11.6 独热编码One-Hot
+ 特征有多个可能值时：构建多个特征，特征的值为1或0(**二进制特征**)![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240326160136.png)
 [[5 Python特征工程#3.1.6.2.独热编码(One-hot Encoding)]]
 + 1指的就是热特征，所以叫独热特征
 + 这对其他模型也很有用，这一点我们早已知道

## 11.7 ID3算法的不足
+ 没有考虑连续特征
+ 没有考虑缺失值
+ 没有考虑过拟合
+ 取值多的特征比取值少的特征信息增益大

## 11.8 C4.5算法
### 11.8.1 C4.5算法的改进
+ **连续特征**：将连续特征离散化
+ **缺失值处理**：
	+ 某些特征缺失的情况下选择划分的属性：
		+ C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。
	+ 选定划分属性对于该属性上缺失特征样本的处理：
		+ 可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。
+ **过拟合**：引入正则化系数进行初步的剪枝
+ **容易偏向于取值较多的特征**：![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240401155740.png)

### 11.8.2 C4.5算法的不足与思考
+ **过拟合剪枝**：
	+ **预剪枝**：生成决策树的时候就决定是否剪枝
	+ **后剪枝**：先生成决策树，再通过交叉验证来剪枝
+ **多叉树**：二叉树比多叉树效率更高
+ **只能用于分类**
+ **运算较慢**

## 11.9 CART算法
### 11.9.1 CART分类树算法的最优特征选择方法
+ 在C4.5算法中，采用了信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算。能不能简化模型同时也不至于完全丢失熵模型的优点呢？有！
+ CART分类树算法使用**基尼系数**来代替信息增益比，基尼系数代表了模型的**不纯度**，基尼系数越小，则不纯度越低，特征越好。这**和信息增益(比)是相反的**。
+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240401160907.png)

+ 比较下基尼系数表达式和熵模型的表达式，二次运算是不是比对数简单很多？尤其是二类分类的计算，更加简单。但是简单归简单，和熵模型的度量方式比，基尼系数对应的误差有多大呢？对于二类分类，基尼系数和熵之半的曲线如下：![1042406-20161111105202170-1563882835.jpg](https://aquazone.oss-cn-guangzhou.aliyuncs.com/1042406-20161111105202170-1563882835.jpg)

+ 基尼系数和熵之半的曲线非常接近，仅仅在45度角附近误差稍大。因此，基尼系数可以做为熵模型的一个近似替代。而CART分类树算法就是使用的基尼系数来选择决策树的特征。同时，为了进一步简化，CART分类树算法每次仅仅对某个特征的值进行二分，而不是多分，这样CART分类树算法建立起来的是二叉树，而不是多叉树。这样一可以进一步简化基尼系数的计算，二可以建立一个更加优雅的二叉树模型。

### 11.9.2 CART分类树算法对于连续与离散特征的处理
+ 其思想和C4.5是相同的，都是将连续的特征离散化。唯一的区别在于在选择划分点时的度量方式不同，C4.5使用的是信息增益比，则CART分类树使用的是基尼系数。
+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240401163838.png)

### 11.9.3 CART分类树建立算法的具体流程
+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240401163930.png)

### 11.9.4 CART回归树建立算法
+ 与CART分类树算法的不同：
	+ 两者的区别在于样本输出，如果样本输出是离散值，那么这是一颗分类树。如果果样本输出是连续值，那么那么这是一颗回归树。
	+ 连续值的处理方法不同
		+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240401164103.png)
	+ 决策树建立后做预测的方式不同
		+ CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果

### 11.9.5 CART树算法的剪枝
#### 11.9.5.1 基本步骤
+ CART回归树和CART分类树的剪枝策略除了在度量损失的时候一个使用**均方差**，一个使用**基尼系数**，算法基本完全一样
+ 剪枝类似于线性回归的正则化[[8 正则化Regularization]]，来增加决策树的泛化能力。CART采用的方法是后剪枝：
	+ 从原始决策树生成各种剪枝效果的决策树
	+ 使用交叉验证来检验各种剪枝的泛化预测能力

#### 11.9.5.2 剪枝的损失函数
+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240401164806.png)![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240401164931.png)
+ 一般来说，α越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的α，一定存在使损失函数Cα(T)最小的唯一子树。
+ **CART树的交叉验证策略**：
	+ 上面我们讲到，可以计算出每个子树是否剪枝的阈值α，如果我们把所有的节点是否剪枝的值α都计算出来，然后分别针对不同的α所对应的剪枝后的最优子树做交叉验证。这样就可以选择一个最好的α，有了这个α，我们就可以用对应的最优子树作为最终结果。

#### 11.9.5.3 剪枝算法实现
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240401165112.png)

## 11.10 CART小结
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240401165233.png)

+ CART的缺陷：
	+ 多变量决策树：OC1算法等
	+ 样本如果发生小改动，树结构将会剧烈改变：随机森林

## 11.11 决策树小结
![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240401165511.png)
