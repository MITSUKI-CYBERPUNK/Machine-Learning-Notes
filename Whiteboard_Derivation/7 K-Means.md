# 7 K-Means
[[13 聚类Clustering]]
## 7.1 K-Means原理
+ **算法思想**：对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。

+ **表达式**：![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240402213450.png)

+ **K-Means的启发式方法**：![1042406-20161212135954464-1143551568.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/1042406-20161212135954464-1143551568.png)

## 7.2 传统K-Means算法流程
+ 我们需要注意的是：
	+ **K值**的选择：一般来说，我们会根据对数据的先验经验选择一个合适的k值，如果没有什么先验知识，则可以通过交叉验证选择一个合适的k值。
	+ **质心**的选择：在确定了k的个数后，我们需要选择k个初始化的质心，就像上图b中的随机质心。由于我们是启发式方法，k个初始化的质心的位置选择对最后的聚类结果和运行时间都有很大的影响，因此需要选择合适的k个质心，最好这些质心不能太近。

+ **传统K-Means算法流程**：![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240402213817.png)

## 7.3 K-Means初始化优化K-Means++
+ 在上节我们提到，k个初始化的质心的位置选择对最后的聚类结果和运行时间都有很大的影响，因此需要选择合适的k个质心。如果仅仅是完全随机的选择，有可能导致算法收敛很慢。**K-Means++算法就是对K-Means随机初始化质心的方法的优化**。
+ K-Means++的对于初始化质心的**优化策略**：![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240402213931.png)

## 7.4 K-Means距离计算优化elkan K-Means 
+ 在传统的K-Means算法中，我们在每轮迭代时，要计算所有的样本点到所有的质心的距离，这样会比较的耗时。那么，对于距离的计算有没有能够简化的地方呢？elkan K-Means算法就是从这块入手加以改进。它的目标是**减少不必要的距离的计算**。那么哪些距离不需要计算呢？
+ elkan K-Means利用了**两边之和大于等于第三边,以及两边之差小于第三边的三角形性质**，来减少距离的计算。
+ ![image.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/20240402214123.png)

## 7.5 大样本优化Mini Batch K-Means

^24bbdd

+ 在传统的K-Means算法中，要**计算所有的样本点到所有的质心的距离**。如果样本量非常大，比如达到10万以上，特征有100以上，此时用传统的K-Means算法非常的耗时，就算加上elkan K-Means优化也依旧。在大数据时代，这样的场景越来越多。此时Mini Batch K-Means应运而生。
+ 顾名思义，Mini Batch，也就是**用样本集中的一部分的样本来做传统的K-Means**，这样可以避免样本量太大时的计算难题，算法收敛速度大大加快。当然此时的代价就是我们的聚类的精确度也会有一些降低。一般来说这个降低的幅度在可以接受的范围之内。
+ 在Mini Batch K-Means中，我们会选择一个合适的**批样本大小batch size**，我们仅仅用batch size个样本来做K-Means聚类。那么这batch size个样本怎么来的？一般是通过**无放回的随机采样**得到的。
+ 为了增加算法的准确性，我们一般会**多跑几次**Mini Batch K-Means算法，用**得到不同的随机采样集来得到聚类簇**，选择其中最优的聚类簇。

## 7.6 K-Means与KNN
+ K-Means是**无监督学习的聚类算法，没有样本输出**；而KNN是**监督学习的分类算法，有对应的类别输出**。**KNN基本不需要训练**，对测试集里面的点，只需要找到在训练集中最近的k个点，用这最近的k个点的类别来决定测试点的类别。而**K-Means则有明显的训练过程**，找到k个类别的最佳质心，从而决定样本的簇类别。
+ 当然，两者也有一些相似点，两个算法都包含一个过程，即**找出和某一个点最近的点**。两者都利用了**最近邻(nearest neighbors)**的思想。

## 7.7 K-Means小结
+ **优点**：
	+ 原理比较简单，实现也是很容易，收敛速度快。
	+ 聚类效果较优。
	+ 算法的可解释度比较强。
	+ 主要需要调参的参数仅仅是簇数k。

+ **缺点**：
	+ K值的选取不好把握
	+ 对于不是凸的数据集比较难收敛
	+ 如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。
	+ 采用迭代方法，得到的结果只是局部最优。
	+ 对噪音和异常点比较的敏感。

## 7.8 评估标准
+ **内部指标**：
	- **轮廓系数（Silhouette Coefficient）**：
```
   S = (b(a - b)) / max(a, b)
```
  + 其中，a是样本与其所在簇中其他样本的平均距离，b是该样本与最近的其他簇的平均距离。S的取值范围在-1到1之间，1表示样本与所在簇内其他样本非常相似，-1表示样本可能属于其他簇，接近0表示样本在两个簇之间。

    - **Calinski-Harabasz指数（Calinski-Harabasz Score）**：
```
CH = (tr(Between\_SS) / k) / (tr(Within\_SS) / (n - k))
```
+ 其中，tr(Between_SS)是簇间差异矩阵的迹，tr(Within_SS)是簇内差异矩阵的迹，k是簇的数量，n是样本总数。值越大，表示簇内的样本更集中，簇间差异更大。

    - **Davies-Bouldin指数（Davies-Bouldin Score）**：
```
DB = (1/k) * Σ[i=1 to k] (max[j≠i](d(Ci, Cj)) / m(Ci))
```
+ 其中，Ci和Cj是不同的簇，d(Ci, Cj)是两个簇中心的距离，m(Ci)是簇Ci中所有样本的平均距离。值越小，表示聚类效果越好。


+ **外部指标**（需要有真实标签或金标准）：
    - **Adjusted Rand Index（ARI）**：
```
    ARI = (RI - E[RI]) / max(RI) - E[RI])
```
+ 其中，RI是原始的rand指数，E[RI]是随机分配时rand指数的期望值。范围在-1到1之间，1表示完全匹配，0表示随机分配，-1表示完全相反。

    - **Adjusted Mutual Information（AMI）**：
```
   AMI = (2 * I(X, Y) - H(X) - H(Y)) / (max(H(X), H(Y)))
```
+ 其中，I(X, Y)是X和Y的互信息，H(X)和H(Y)是X和Y的熵。范围在-1到1之间，1表示完美匹配，0表示无相关性，负值表示有误导性。

    - **Fowlkes-Mallows Index（FMI）**：
```
 FMI = sqrt((TP / (TP + FP)) * (TP / (TP + FN)))
```
+ 其中，TP是真正例，FP是假正例，FN是假反例。范围在0到1之间，1表示完美匹配，0表示聚类效果差。