# 1 Linear Regression

## 1.1 数据清洗


```python
# 导入依赖库
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# 忽略警告
import warnings
warnings.filterwarnings('ignore')

#导入数据
train=pd.read_csv(r'D:\RDC\insurance.csv')
ym = train['charges'].mean()
ys = train['charges'].std()
```


```python
# 查看数据集信息
train.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 1338 entries, 0 to 1337
    Data columns (total 7 columns):
     #   Column    Non-Null Count  Dtype  
    ---  ------    --------------  -----  
     0   age       1338 non-null   int64  
     1   sex       1338 non-null   object 
     2   bmi       1338 non-null   float64
     3   children  1338 non-null   int64  
     4   smoker    1338 non-null   object 
     5   region    1338 non-null   object 
     6   charges   1338 non-null   float64
    dtypes: float64(2), int64(2), object(3)
    memory usage: 73.3+ KB



```python
# 查看前五行
train.head()
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>sex</th>
      <th>bmi</th>
      <th>children</th>
      <th>smoker</th>
      <th>region</th>
      <th>charges</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>19</td>
      <td>female</td>
      <td>27.900</td>
      <td>0</td>
      <td>yes</td>
      <td>southwest</td>
      <td>16884.92400</td>
    </tr>
    <tr>
      <th>1</th>
      <td>18</td>
      <td>male</td>
      <td>33.770</td>
      <td>1</td>
      <td>no</td>
      <td>southeast</td>
      <td>1725.55230</td>
    </tr>
    <tr>
      <th>2</th>
      <td>28</td>
      <td>male</td>
      <td>33.000</td>
      <td>3</td>
      <td>no</td>
      <td>southeast</td>
      <td>4449.46200</td>
    </tr>
    <tr>
      <th>3</th>
      <td>33</td>
      <td>male</td>
      <td>22.705</td>
      <td>0</td>
      <td>no</td>
      <td>northwest</td>
      <td>21984.47061</td>
    </tr>
    <tr>
      <th>4</th>
      <td>32</td>
      <td>male</td>
      <td>28.880</td>
      <td>0</td>
      <td>no</td>
      <td>northwest</td>
      <td>3866.85520</td>
    </tr>
  </tbody>
</table>



```python
# 描述性统计
train.describe()
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>bmi</th>
      <th>children</th>
      <th>charges</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1338.000000</td>
      <td>1338.000000</td>
      <td>1338.000000</td>
      <td>1338.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>39.207025</td>
      <td>30.663397</td>
      <td>1.094918</td>
      <td>13270.422265</td>
    </tr>
    <tr>
      <th>std</th>
      <td>14.049960</td>
      <td>6.098187</td>
      <td>1.205493</td>
      <td>12110.011237</td>
    </tr>
    <tr>
      <th>min</th>
      <td>18.000000</td>
      <td>15.960000</td>
      <td>0.000000</td>
      <td>1121.873900</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>27.000000</td>
      <td>26.296250</td>
      <td>0.000000</td>
      <td>4740.287150</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>39.000000</td>
      <td>30.400000</td>
      <td>1.000000</td>
      <td>9382.033000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>51.000000</td>
      <td>34.693750</td>
      <td>2.000000</td>
      <td>16639.912515</td>
    </tr>
    <tr>
      <th>max</th>
      <td>64.000000</td>
      <td>53.130000</td>
      <td>5.000000</td>
      <td>63770.428010</td>
    </tr>
  </tbody>
</table>


### 1.1.1 缺失值处理


```python
# 查看缺失值
train.isnull().sum()
```


    age         0
    sex         0
    bmi         0
    children    0
    smoker      0
    region      0
    charges     0
    dtype: int64

我们发现数据集中没有缺失值，因此无需处理缺失值



### 1.1.2 重复值处理


```python
# 检查重复行
dup = train.duplicated()
print(dup.sum())

# 删除重复行
train.drop_duplicates(inplace=True)
```

    1

```python
# 利用箱线图查看连续性数值列是否存在离群值
data = ['age', 'bmi']

plt.subplots(figsize=(17, 7))
for i, col in enumerate(data):
    plt.subplot(1, 2, i + 1)
    sb.boxplot(train[col])
plt.show()
```

![output_11_0.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/output_11_0.png)

```python
# 查看离群值数目
train[train['bmi']>45].shape
```


    (20, 7)




```python
# 离群值为20，对整体数据的影响不大，我们可以直接删除
train = train[(train['bmi'] < 45)]
              
data = ['bmi']

plt.subplots(figsize=(17, 7))
for i, col in enumerate(data):
    plt.subplot(1, 2, i + 1)
    sb.boxplot(train[col])
plt.show()
```

![output_9_0.png](https://aquazone.oss-cn-guangzhou.aliyuncs.com/output_9_0.png)

根据箱线图，处理后的数据集中没有离群值



### 1.1.3 独热编码处理


```python
# 利用Pandas里的get_dummies()进行独热编码处理
train_encoded = pd.get_dummies(train, columns=['sex', 'smoker', 'region'])

train = train_encoded

# 强制类型转换
columns_to_convert = ['sex_female', 'sex_male', 'smoker_no', 'smoker_yes', 'region_northeast', 'region_northwest', 'region_southeast', 'region_southwest']

for column in columns_to_convert:
    train[column] = train[column].astype(float)

# 检查转换后的数据类型
print(train.dtypes)
```

    age                   int64
    bmi                 float64
    children              int64
    charges             float64
    sex_female          float64
    sex_male            float64
    smoker_no           float64
    smoker_yes          float64
    region_northeast    float64
    region_northwest    float64
    region_southeast    float64
    region_southwest    float64
    dtype: object



### 1.1.4 标准化处理


```python
# 标准化处理
train = (train - train.mean()) / train.std()
```



### 1.1.5 训练前准备


```python
# 将特征X和目标变量y分离
X = train.drop('charges', axis=1)
y = train['charges']

# 分割数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 将 DataFrame 转换为 NumPy 数组
X_train = X_train.values
X_test = X_test.values
```



## 1.2 模型训练

### 1.2.1 线性回归正规方程法


```python
# 线性回归正规方程法
def normal_equation(X, y):
    # 添加截距项(将全1数组与原矩阵水平堆叠)
    X = np.hstack((np.ones((X.shape[0], 1)), X))


    # 计算参数，使用伪逆代替标准逆
    theta = np.linalg.pinv(X.T @ X) @ X.T @ y
    return theta

# 模型训练
theta_normal = normal_equation(X_train, y_train)

# 为测试数据 X_test 添加一个截距列
X_test_intercept = np.hstack((np.ones((X_test.shape[0], 1)), X_test))

# 利用参数
y_pred_normal = X_test_intercept.dot(theta_normal)
```


```python
mse_normal = mean_squared_error(y_test, y_pred_normal)

# 计算每个特征的范围
feature_ranges = X.max(axis=0) - X.min(axis=0)

# 计算标准化均方误差（NMSE）
normalized_mse = mse_normal / (feature_ranges.max() ** 2)

r2_normal = r2_score(y_test, y_pred_normal)

print(f'正规方程法: NMSE = {normalized_mse:.2f}, R^2 = {r2_normal:.2f}')

# 反标准化预测结果（以使结果回到正常范围）
y_pred_normal_ = y_pred_normal * ys + ym

print(f'预测医疗保险费用: {y_pred_normal_}')
```

    正规方程法: NMSE = 0.01, R^2 = 0.79
    预测医疗保险费用: [34672.79494631 34473.40278405 15967.81067849 32256.07490018
      3992.72295907  8298.03872178 10442.92444548  9858.8945769
      7358.8574003  11678.38685971  1542.8352028  11470.28133516
     15524.70350833  3290.26079099 33564.04146846 10235.22743738
       926.64332665  9721.58009849 -2584.89505062  9078.12257592
      1667.38040522 11854.30166798 11762.12890567 28374.06672431
      7500.1933335   3485.82414003 13253.15939601  6872.02494284
      5830.28829765  9929.22210666 11122.53994706  7010.21144077
      8078.82288635  4636.81383602  4814.71779554 10263.50240531
      -776.77400029  3936.07872386 13636.51327543  8690.44094662
      4444.40961847 11507.4307239   8915.0433797  15146.41967983
     11217.63834658 10692.25123002  4098.19998214  6187.12297634
      5566.20103012 12248.304715    6466.24664751 13363.03150602
      -117.61312985  4177.57798509  6968.85974053 11529.94001342
      3481.03063602 10656.19546924  1540.30677484  7135.15196878
     11653.62822059 29216.55649234 10712.85381314 10109.43836625
     12365.67687478  6236.98186162  9756.5707195   4907.57480221
       694.43678878 13040.0004824   1269.99359973 11682.15613849
     36500.85167663 27952.86030396 10159.90315502 11270.56215065
      1435.76773965 12249.19250794 11639.14821181 17080.02042466
     32046.1753596  13062.58542203  8052.48370967 22529.47213986
     31541.82394811  7844.46214934 10652.62603601 36984.3910231
     15572.83800183  7336.52159333 12892.2146734  11482.01733365
      6253.54727837  6499.7699325   -144.78642204  4428.03095107
      4393.80015276  2920.7510242  40079.46605097 13784.16946254
     12827.06713231 -2295.11755223 13534.85587143  7303.91275863
     30414.75004345  7493.78548155  9778.29097231 32987.04624115
      5146.64123839 11961.3407883   3855.70820921 36033.48231978
     26672.29510602  8801.92947025 12804.76678673  2769.80121844
     36867.37540189 15199.58359582  5605.26218583  8748.29483648
      -690.35223436 16344.28701398 13385.10082734 14625.79662688
      2860.54157716 13361.29975003  4852.08038351  8409.62940493
     36736.4617709  37082.80260696  5561.3664662   3846.87767387
      7209.98388688 10658.39831915 11865.29081643  3754.63104674
      6452.0963854  13040.97500625 34253.59670832  4728.11629966
      6023.3724325  12804.99781099 14267.0649543  15120.02467431
      7465.55733098 14162.56540119  8322.65667333 16377.47586787
     36450.63824488 12953.79273405 12030.30382862  7323.53694595
      9067.48084431  5059.86225033 11338.61032328  7952.29877743
     33329.35208673  9681.22864617  7372.86240863 38701.83831943
     10061.184914   34692.53069409  9081.05314142  4344.46271502
      8314.11284234  8176.69852906  6634.18184575 11166.51553842
      1682.92653494 31840.52886028  4286.55122136  8266.53974539
      5596.86822598  1429.61709297  9310.8801661  25525.16945004
      5468.61739031 30900.21433952 25439.17179198 12035.66141971
     15233.61253103 33298.77605649 10379.27960107  6650.1792035
       509.9615705   9133.30850992 15024.99074924  3048.12723648
     10470.6966472  10558.37482645  5311.8019321  30974.20342474
      7615.8077191  29102.04911169 13066.71413736 14163.64977613
     12565.11811694  6254.62445191 10889.37708064 12985.50916496
      6830.21130172 18136.90803681 33942.39966334 11233.9252713
     34450.27058435 35371.446226    6051.37565849  5247.28710646
      7046.74409494  4061.50110891 10364.44763636 12773.48324317
      6646.80874611 27720.7252966   8854.39766771  7128.92606119
     11157.21117636  9110.99407897 33565.99051616  7048.72060501
     15692.63018267  4021.86603603  2788.680362   12239.8487507
     13242.67885446  4332.00322234  1808.56399727 11940.49787274
      3607.49987889  3445.27030037 32596.80192106 39499.62275686
      6887.32246479 28579.14278739 38998.28688781  1998.93316204
      9236.1657766  14118.34346459 40699.37748707 25740.79239073
     14865.57483166 10217.57716958 23494.62090286  7460.47374438
     38749.86296183  8662.00435716  5216.61819774 10441.16833604
     15603.87670207  8183.74501648 13032.1843422  36019.272413
     10100.95358303 34144.86333425 35883.02810362  4131.4706594
      7148.62444135  7477.98312773 25370.43449484 -1354.93978119
     14963.49955428  6164.33267781  9943.89450402  2113.76068096]


NMSE = 0.01 表示模型的预测误差相对较小，预测精度较高。
R^2 = 0.79 表示模型能够解释大约 79% 的观测数据方差，模型整体的拟合效果较好。



### 1.2.2 带正则化的线性回归正规方程法


```python
# 带正则化的线性回归正规方程法
def normal_equation_with_regularization(X, y, lambda_reg):  
    # 添加截距项  
    X_with_intercept = np.hstack((np.ones((X.shape[0], 1)), X))
      
    # 计算正则化矩阵，不包括截距项  
    num_features = X_with_intercept.shape[1]
    reg_matrix = lambda_reg * np.eye(num_features)
    reg_matrix[0, 0] = 0  # 截距项不进行正则化
      
    # 计算带正则化的参数  
    theta = np.linalg.inv(X_with_intercept.T @ X_with_intercept + reg_matrix) @ X_with_intercept.T @ y
      
    return theta

# 使用带有正则化的正规方程法训练模型
lambda_reg = 500 # 这里选择500是为了突出正则化参数的作用

# 训练模型
theta_normal_re = normal_equation_with_regularization(X_train, y_train, lambda_reg)

# 为测试数据集添加截距项
X_test_intercept = np.hstack((np.ones((X_test.shape[0], 1)), X_test))
y_pred_normal_re = X_test_intercept.dot(theta_normal_re)
```


```python
# 获取特征的最大值和最小值
feature_max = X_train.max(axis=0)
feature_min = X_train.min(axis=0)

mse_normal_re = mean_squared_error(y_test, y_pred_normal_re)

# 计算每个特征的范围
feature_ranges = feature_max - feature_min

# 选择一个统一的范围用于标准化MSE，这里选择最大范围
max_range = feature_ranges.max()

# 计算标准化均方误差（NMSE）
normalized_mse_re = mse_normal_re / max_range**2

r2_normal_re = r2_score(y_test, y_pred_normal_re)

print(f'正则化的正规方程法: NMSE = {normalized_mse_re:.2f}, R^2 = {r2_normal_re:.2f}')

# 反标准化预测结果
y_pred_normal_re_ = y_pred_normal_re * ys + ym

print(f'预测医疗保险费用: {y_pred_normal_re_}')
```

    正则化的正规方程法: NMSE = 0.01, R^2 = 0.74
    预测医疗保险费用: [30293.38543696 30045.25016847 14953.62479797 28899.53580589
      6473.43119225  9422.31963623 10455.81826142 10368.50590212
      8339.57443071 11667.87292389  4713.12818381 11526.89273082
     13818.19728798  5585.14842599 29490.52054917 10308.91961806
      4298.12422027 10383.89001302  1588.92978557 10309.37262145
      5153.57315853 11690.85453386 11721.66679301 25837.37352644
      8452.65189184  5751.99139575 12732.0730624   8402.57818566
      7356.50775102 10626.66887419 11351.20407937  8636.74390372
      9155.5416537   6509.7590885   7034.85344787 10358.00727394
      3188.07467674  6502.70767729 12959.75723053  9599.7174936
      6566.22646797 11302.28381982  9616.18188234 14085.17801473
     11346.19768956 10636.65553789  6070.90573603  7917.91681721
      7115.96991092 12006.81865529  7785.29500533 12659.62178428
      3637.14598401  6860.63781261  8510.28188188 11415.44751546
      5761.31369403 11018.66927978  4696.55473997  8514.60414505
     11646.26529603 26876.02197563 11053.2371261  10573.59409129
     11866.23557366  7907.81467495 10347.74966464  7036.71517996
      4176.4088317  13011.73224696  4140.30929323 11518.15468224
     30962.61281292 25341.34453765 10425.37309057 11042.05247588
      4323.18918821 11849.11370974 11221.75755577 14998.43955724
     28453.39655176 12662.95102071  9241.54853315 21549.16445203
     27727.69416534  9154.09963086 10983.27556741 32176.32206021
     14274.13545698  8408.60175191 12457.14173419 11843.38202071
      7966.58492219  7966.40127009  3970.37600933  6670.0494706
      6758.03466251  6005.12254009 34186.83151728 12788.1754013
     12088.93347525  1785.9935263  12875.27862796  8801.0866343
     27585.2457715   8496.65687996 10026.42291401 29027.91323997
      7199.29276606 11858.41555038  6230.8801107  31178.75320702
     24376.19150969  9661.48919477 12061.65258979  5535.097881
     31772.89087099 14021.918485    7469.3534817   9770.10283994
      3546.96177993 15324.95065755 12785.55291737 13570.76877199
      5179.25571694 13240.73252084  7010.283888    9761.26804289
     31121.22450806 31466.86940399  7330.66130078  5952.39467843
      8734.78702726 11040.40987681 11886.85643929  6263.15367288
      8114.58187029 12964.03231369 29431.13198547  6972.72913725
      7699.67359603 11963.00234216 13066.5913428  13646.18159106
      9106.32683512 13363.70487747  9031.66983423 14519.05739176
     30923.61887568 12213.31709135 11750.87468561  8266.72539134
      9624.90646867  7044.4366581  11640.49264253  8827.36220096
     29262.85674849 10367.80678082  8992.20619945 32869.21236471
     10193.45140344 30193.38509324  9528.86738289  6676.43537387
      9411.39292879  9164.85481643  8190.04677498 11257.75603216
      4421.12512377 28311.98091501  6373.94154587  9442.48437097
      7569.17943915  4304.15244761 10156.99389802 24256.03866116
      7480.88918058 27334.88278963 23886.88982292 11811.76548052
     13615.39411626 28919.6230637  10358.51990678  8182.03332943
      3992.97678218 10037.04365392 14474.50797629  5615.87427023
     10621.87481639 10913.47589924  6838.16300347 27985.0878996
      8686.0560499  26858.62916589 12246.25198164 13356.09997993
     12375.43318928  7902.53547198 11500.83403877 12507.34824867
      8840.34672348 15703.13171254 29686.4878525  11021.17599379
     29729.32752651 31128.60623276  7767.07990336  6957.14694298
      8211.53801118  6783.31491052 10470.17173385 11987.51036167
      8119.60415309 25818.49345186  9847.36980347  8198.89653841
     11253.85168157  9619.37364939 29395.12068264  8565.13378116
     14351.90292311  6083.48566843  5709.98634887 11579.47934484
     13151.17945967  6342.44232373  5250.39302924 11913.91952374
      6117.20733652  6105.98116423 28436.24925413 33124.47884069
      8366.23403363 26146.47025057 33132.90633133  5058.68788238
      9679.48388839 13755.5615435  34716.43658581 23931.64894451
     13578.41144236 10611.62098417 22558.84038356  8629.42212486
     33285.05549799  9666.28897286  7309.78198234 10358.70639523
     14347.10217601  9251.09946181 12597.41017993 31198.82524718
     10277.74351755 29925.47984201 30527.11442218  6143.50958271
      8367.53947087  8481.06426194 23727.34131124  2742.49077651
     13848.78695223  7597.72155122 10450.51557805  4978.84111217]


NMSE = 0.01, R^2 = 0.74，相对于不带正则化的正规方程法有所下降，这说明模型过拟合



### 1.2.3 线性回归梯度下降法


```python
# 线性回归梯度下降法
def gradient_descent(X, y, alpha=0.01, num_iters=1000):
    # 添加截距项
    X = np.hstack((np.ones((X.shape[0], 1)), X))

    # 初始化参数
    theta = np.zeros(X.shape[1])

    # 梯度下降
    for i in range(num_iters):
        hypothesis = X @ theta
        gradient = (1 / X.shape[0]) * X.T @ (hypothesis - y)
        theta = theta - alpha * gradient

    return theta

theta_gd = gradient_descent(X_train, y_train,alpha=0.01, num_iters=1000)

# 为测试数据集添加截距项
X_test_intercept = np.hstack((np.ones((X_test.shape[0], 1)), X_test))
y_pred_gd = X_test_intercept.dot(theta_gd)
```


```python
# 获取特征的最大值和最小值
feature_max = X_train.max(axis=0)
feature_min = X_train.min(axis=0)

mse_gd = mean_squared_error(y_test, y_pred_gd)

# 计算每个特征的范围
feature_ranges = feature_max - feature_min

# 选择一个统一的范围用于标准化MSE，这里选择最大范围
max_range = feature_ranges.max()

# 计算标准化均方误差（NMSE）
normalized_mse_gd = mse_gd / max_range**2

r2_gd = r2_score(y_test, y_pred_gd)

print(f'梯度下降法: NMSE = {normalized_mse_gd:.2f}, R^2 = {r2_gd:.2f}')

# 反标准化预测结果
y_pred_gd_ = y_pred_gd * ys + ym

print(f'预测医疗保险费用: {y_pred_gd_}')
```

    梯度下降法: NMSE = 0.01, R^2 = 0.79
    预测医疗保险费用: [34672.91531985 34473.35168053 15967.4677557  32256.11657251
      3993.108334    8297.91172459 10442.97859565  9858.77930849
      7358.20001989 11678.47682086  1542.94147094 11470.13915552
     15524.32869077  3290.11954552 33563.81403288 10235.07767407
       926.99561478  9721.80786681 -2584.29804825  9078.33385534
      1667.87798158 11854.53890938 11762.14009372 28374.05018212
      7500.21093696  3485.87990653 13252.4859347   6872.0721229
      5830.10881406  9929.51471902 11122.37241372  7010.68909868
      8078.84222374  4636.8591008   4815.09433823 10263.70070645
      -776.25111798  3936.4052754  13636.47000085  8690.66571763
      4444.15345602 11507.42572593  8914.59663282 15146.43067522
     11217.16605135 10691.86483632  4098.24838314  6187.01384203
      5565.93763942 12248.0803986   6466.17535774 13362.76042942
      -117.14375464  4177.74602998  6969.38282677 11530.00865663
      3481.25863305 10656.26751833  1540.3322767   7135.32821417
     11653.58760105 29216.74600461 10712.81761898 10109.84013722
     12365.2799568   6236.57097263  9756.09207527  4907.46377049
       695.07699936 13040.02191417  1270.11604889 11682.17177359
     36500.67537542 27952.65783334 10159.65874922 11270.59522229
      1435.83692105 12249.31929174 11639.13137633 17079.52202676
     32046.75488024 13062.37933752  8052.63336154 22529.7904564
     31541.48553787  7844.8172635  10652.57321714 36984.28736018
     15572.44145529  7336.75388394 12891.88784771 11481.85395966
      6253.85401245  6499.34302515  -144.07094955  4428.0736683
      4394.33577386  2921.05103607 40079.16120346 13783.87053383
     12826.94687474 -2294.558591   13534.19845535  7304.12744687
     30414.52502955  7493.7509007   9778.48691937 32986.86173446
      5146.49882271 11961.03506694  3856.02022074 36033.29398498
     26672.4658722   8802.09153278 12804.15407237  2769.76439268
     36866.67926732 15199.30210028  5605.441117    8748.30486344
      -689.67724416 16344.54820671 13384.95845498 14625.47188071
      2860.73942865 13361.70833563  4852.43899563  8410.16659573
     36736.18848824 37082.5824335   5561.16182661  3847.07117031
      7210.11182897 10658.48987072 11865.0520366   3755.10070321
      6452.09961559 13041.04933561 34253.58331776  4728.37210866
      6023.31832155 12804.74884918 14266.68960674 15119.86113575
      7465.6900469  14162.59345153  8322.84555812 16377.00364674
     36450.27038182 12953.53029041 12030.10863179  7323.26101091
      9067.58295521  5059.9346262  11338.66963439  7952.45023059
     33328.92923409  9681.11822153  7373.68179204 38701.15304216
     10060.83329228 34692.35347568  9081.25304849  4344.54447554
      8313.89789208  8176.39476131  6634.21419781 11166.22860931
      1682.99477546 31840.26347748  4286.76759591  8266.61301577
      5597.24116709  1429.60598359  9311.2739525  25525.42119803
      5468.68165629 30899.70897868 25439.64789221 12035.77577586
     15233.07777333 33298.43170487 10379.16395252  6649.96322916
       510.34703931  9133.75863303 15025.3060586   3048.02134831
     10470.33034312 10558.90905012  5311.34773301 30974.69758232
      7616.04737593 29102.69601496 13066.33124029 14163.05494201
     12565.02338984  6254.03107178 10889.73386555 12985.15504487
      6831.12701362 18136.22282118 33942.45302245 11234.12828062
     34449.99474169 35371.53430591  6051.26484583  5247.40886379
      7047.0144267   4061.75044327 10364.38150646 12773.08631612
      6646.50766414 27721.49445899  8854.88440543  7129.283161
     11157.02454557  9111.15855007 33565.86887576  7048.97374707
     15692.40959075  4021.66177317  2789.17551933 12239.70700565
     13242.73973044  4332.21858301  1809.07606508 11940.48716523
      3607.33510382  3445.92569703 32595.96173452 39499.13338282
      6887.48671339 28579.41765251 38998.07110368  1999.20396838
      9236.16019293 14118.65266769 40699.34756704 25740.82480764
     14865.34511863 10217.38178873 23495.12109876  7460.31702606
     38749.83173764  8662.2108401   5217.00803135 10441.36179341
     15603.84764372  8183.87117696 13032.45986256 36019.07917771
     10100.9069969  34144.6897313  35882.3054217   4131.52771844
      7148.89240302  7477.75246764 25370.42948011 -1354.45310152
     14963.07744993  6164.15741443  9943.88867403  2113.7114025 ]


NMSE = 0.01 表示模型的预测误差相对较小，预测精度较高。 R^2 = 0.79 表示模型能够解释大约 79% 的观测数据方差，模型整体的拟合效果较好。评估结果与正规方程法类似，说明模型拟合得还算可以。



### 1.2.4 线性回归的带正则化的梯度下降法


```python
# 线性回归的带正则化的梯度下降法
def gradient_descent_with_regularization(X, y, alpha=0.01, lambda_reg=0.0, num_iters=1000, tolerance=1e-6):
    # 添加截距项
    X = np.hstack((np.ones((X.shape[0], 1)), X))
    
    # 初始化参数
    theta = np.zeros(X.shape[1])
    
    m = X.shape[0]  # 训练样本数量
    
    # 梯度下降
    for i in range(num_iters):
        hypothesis = X @ theta
        errors = hypothesis - y
        
        # 添加 L2 正则化项,注意不要正则化截距项
        gradient = (1 / m) * (X.T @ errors + lambda_reg * np.r_[0, theta[1:]])
        
        theta_new = theta - alpha * gradient
        
        # 检查收敛条件
        if np.linalg.norm(theta_new - theta) < tolerance:
            break
        
        theta = theta_new
    
    return theta

# 定义正则化参数
lambda_reg = 500 # 这里选择500是为了突出正则化参数的作用

# 训练模型
theta_gd_reg = gradient_descent_with_regularization(X_train, y_train, alpha=0.01, lambda_reg=lambda_reg, num_iters=1000)

# 为测试数据集添加截距项
X_test_intercept = np.hstack((np.ones((X_test.shape[0], 1)), X_test))
y_pred_gd_re = X_test_intercept.dot(theta_gd_reg)

# 计算均方误差和 R^2 值
mse_gd_reg = mean_squared_error(y_test, y_pred_gd_reg)
r2_gd_reg = r2_score(y_test, y_pred_gd_reg)

# 计算标准化均方误差
normalized_mse_gd_reg = mse_gd_reg / max_range**2

# 打印结果
print(f'带正则化的梯度下降法: NMSE = {normalized_mse_gd_reg:.2f}, R^2 = {r2_gd_reg:.2f}')

# 反标准化预测结果
y_pred_gd_re_ = y_pred_gd_re * ys + ym

print(f'预测医疗保险费用: {y_pred_gd_re_}')
```

    带正则化的梯度下降法: NMSE = 0.01, R^2 = 0.74
    预测医疗保险费用: [30293.41168471 30045.08562927 14953.10215438 28900.02642233
      6474.98661193  9422.7221257  10455.8669786  10368.52550026
      8339.13582137 11668.10631567  4714.34230676 11526.62972519
     13816.99643099  5585.85129577 29490.3895267  10308.79758043
      4299.64873659 10384.60424235  1591.04890501 10310.32460124
      5155.46662585 11690.85937249 11721.69149731 25837.68815377
      8452.8442774   5752.99234692 12731.08530457  8403.20672296
      7356.82491131 10627.64613946 11351.22751999  8638.25033439
      9155.90483941  6510.47886179  7036.30140187 10358.19639146
      3190.08099393  6504.18376397 12959.21542234  9600.18465505
      6566.53703764 11302.35375203  9615.77111936 14085.0308618
     11345.64818191 10636.23169123  6071.60008498  7918.32174552
      7116.30103978 12006.27020976  7785.81506807 12658.84403665
      3639.02155443  6861.91263166  8511.61111802 11415.21665884
      5762.30495654 11019.12146331  4697.53561065  8515.23160833
     11646.2533286  26877.13344392 11053.45559095 10574.06030204
     11865.66987575  7907.96702487 10347.37000648  7037.57096484
      4178.49319784 13012.02486685  4141.41531026 11517.85460526
     30961.68204721 25341.84999246 10425.48834628 11041.9616934
      4324.31739917 11849.17771009 11221.48117806 14997.13895303
     28454.36560298 12662.54780867  9242.38876324 21550.59719422
     27727.36801031  9155.27505809 10983.35283338 32176.2166483
     14273.02131554  8409.28886769 12456.57335164 11843.45904693
      7967.67969622  7966.3004062   3972.80978535  6670.69912305
      6759.3874344   6006.67540311 34185.9344563  12787.42006595
     12088.35203478  1788.04122892 12874.15945809  8802.04976559
     27585.58919962  8496.9128316  10026.66820167 29027.92259524
      7200.08963206 11857.93640293  6232.5145038  31178.36234784
     24376.98370952  9661.72671009 12060.6010541   5535.87085608
     31771.86895826 14020.95974577  7470.16775597  9770.53953421
      3549.30949559 15325.1884434  12784.94624645 13569.90032807
      5180.12969278 13241.35813431  7011.59723485  9762.38762617
     31120.17226313 31465.87661167  7330.88682843  5953.371484
      8735.67818945 11040.73794349 11886.977505    6264.70578444
      8115.21878243 12964.26270659 29430.62824044  6974.07161021
      7700.10089594 11962.24982498 13065.59159693 13645.4580911
      9107.17179145 13363.57968214  9032.08363232 14517.86652031
     30922.51024878 12212.87239395 11750.33027582  8266.7199613
      9625.21703658  7045.10141896 11640.89050804  8827.89755598
     29262.29746742 10368.05861408  8994.22281822 32867.81689944
     10192.98091627 30193.19606759  9529.22003264  6677.40148429
      9411.85161851  9164.78312657  8190.5768246  11257.45336188
      4422.12937182 28312.02101048  6375.09355901  9443.03979572
      7570.52974729  4305.048299   10158.02986322 24257.42861563
      7481.65999863 27334.5865122  23888.44232132 11811.63050406
     13614.07476332 28919.25524379 10358.30038009  8182.1670306
      3994.60292858 10038.15508846 14475.06948266  5616.4972532
     10621.67960038 10914.27067722  6838.11579554 27986.05439416
      8686.78637531 26860.30643304 12245.38953663 13355.07806459
     12375.30968513  7902.35448014 11501.59470161 12506.58641007
      8842.358582   15701.3684648  29686.61052399 11021.25277103
     29728.86721545 31128.99203048  7767.5622886   6957.99912927
      8212.29654379  6784.68174935 10470.20772335 11986.73247996
      8119.71927421 25820.22864774  9848.5499767   8199.63517947
     11253.64642263  9619.78196972 29394.86520616  8565.89564941
     14351.11889535  6084.03993725  5711.60645931 11578.89781699
     13151.48554084  6343.31897888  5252.28340738 11913.99502401
      6117.91261457  6107.86174231 28435.31432864 33123.09258268
      8366.86022613 26147.53464496 33132.03879535  5060.26051366
      9679.7334725  13756.00058051 34715.95460397 23932.27750667
     13577.7052258  10611.52047747 22560.65018175  8629.46496464
     33284.58124127  9666.81177127  7311.19429854 10358.66327764
     14346.74858805  9251.5329701  12597.65868979 31198.68686302
     10277.67398297 29925.28743372 30525.733506    6144.32113096
      8368.35322939  8481.13387163 23728.09143033  2744.41906773
     13847.8972669   7598.15855427 10450.61040868  4979.63096137]

NMSE = 0.01, R^2 = 0.74，相对于不带正则化的梯度下降法有所下降，这说明模型过拟合



## 1.3 改进反思

+ 对于数据清洗还是不够熟悉，在这方面用的时间反而最多

+ 模型的拟合度还不够高，要努力学习更深层次的优化方法

+ 用类来表示模型构建的过程会更简洁可读一些

+ 模型评估的标准可以更丰富些

  
